{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>a)</h5>\n",
    "Se cargan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(seed=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>b)</h5>\n",
    "Se genera un boxplot donde se observa que se cumple la ley Zipf, ya que existen varios otulayers hacía arriba, los cuales corresponden al pequeño conjunto de palabras que son altamente utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27bfdcb7f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "from matplotlib import pyplot\n",
    "print(\"Review length: \")\n",
    "result = list(map(len, X))\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un split de los datos creando un dataset solo con clase 0 y un dataset solo con clase 1, al obserbar el boxplot, los resultados son similares a los obtenidos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length with class 0: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20628064cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length with class 1: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2062812b828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "X_0, y_0, X_1, y_1 = [], [], [], []\n",
    "for idx, val in enumerate(y):\n",
    "    if int(val) == 0:\n",
    "        X_0.append(X[idx])\n",
    "        y_0.append(val)\n",
    "    else:\n",
    "        X_1.append(X[idx])\n",
    "        y_1.append(val)\n",
    "\n",
    "        \n",
    "from matplotlib import pyplot\n",
    "print(\"Review length with class 0: \")\n",
    "result = list(map(len, X_0))\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()\n",
    "\n",
    "print(\"Review length with class 1: \")\n",
    "result = list(map(len, X_1))\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>c)</h5>\n",
    "Se cargan los datos extrayendo solo las 3000 palabras más relevantes. Además los comentarios con un largo menor se rellenan con ceros para que todos los vectores de datos sean de la misma forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=3000, seed=15)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>d)</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 345s - loss: 0.5317 - acc: 0.7236 - val_loss: 0.3992 - val_acc: 0.8224\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 339s - loss: 0.3619 - acc: 0.8490 - val_loss: 0.3641 - val_acc: 0.8438\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 338s - loss: 0.3141 - acc: 0.8716 - val_loss: 0.3263 - val_acc: 0.8622\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "embedding_vector_length = 32\n",
    "top_words = 3000\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>e)</h5>\n",
    "Al cambiar la dimención de entrada del embedding a 2000 el algoritmo diverge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 377s - loss: 3.8935 - acc: 0.2885 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 376s - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 364s - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=3000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "embedding_vector_length = 32\n",
    "top_words = 2000\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al aumentar la dimensión inicial del embedding a 4000, se obtiene un resultado con un valor de accuracy pequeñamente más alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 370s - loss: 0.5063 - acc: 0.7402 - val_loss: 0.5019 - val_acc: 0.7570\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 369s - loss: 0.3346 - acc: 0.8596 - val_loss: 0.3851 - val_acc: 0.8443\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 366s - loss: 0.2787 - acc: 0.8887 - val_loss: 0.3022 - val_acc: 0.8764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=3000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "embedding_vector_length = 32\n",
    "top_words = 4000\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al utilizar una dimensión inicial de 6000 el resultado es similar al caso anterior (con 4000) tanto en accuracy como en tiempos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 353s - loss: 0.4939 - acc: 0.7466 - val_loss: 0.3110 - val_acc: 0.8666\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 349s - loss: 0.3184 - acc: 0.8692 - val_loss: 0.3125 - val_acc: 0.8682\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 373s - loss: 0.2741 - acc: 0.8904 - val_loss: 0.3023 - val_acc: 0.8754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=3000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "embedding_vector_length = 32\n",
    "top_words = 6000\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al disminuir el valor a 2500 los tiempos aumentan un poco, y la accuracy se mantiene casi igual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 375s - loss: 0.5402 - acc: 0.7122 - val_loss: 0.4026 - val_acc: 0.8170\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 371s - loss: 0.3543 - acc: 0.8520 - val_loss: 0.3274 - val_acc: 0.8604\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 371s - loss: 0.3327 - acc: 0.8584 - val_loss: 0.3255 - val_acc: 0.8683\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=3000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "embedding_vector_length = 32\n",
    "top_words = 2500\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>f</h5>\n",
    "\n",
    "Se utilizan las 5000 palabras más frecuentes. el accuracy aumenta un poco, por lo cual disminuye el error de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 341s - loss: 0.4746 - acc: 0.7564 - val_loss: 0.3481 - val_acc: 0.8528\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 332s - loss: 0.2923 - acc: 0.8826 - val_loss: 0.3184 - val_acc: 0.8692\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 332s - loss: 0.2647 - acc: 0.8950 - val_loss: 0.3366 - val_acc: 0.8734\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "embedding_vector_length = 32\n",
    "top_words = 5000\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al utilizar solo las 2000 palabras más significativas, los resultados son muy similares a los obtenidos en las 2 situaciones anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 362s - loss: 0.5903 - acc: 0.6902 - val_loss: 0.5314 - val_acc: 0.7388\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 369s - loss: 0.3829 - acc: 0.8340 - val_loss: 0.3436 - val_acc: 0.8546\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 369s - loss: 0.3172 - acc: 0.8711 - val_loss: 0.3383 - val_acc: 0.8608\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=2000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "embedding_vector_length = 32\n",
    "top_words = 2000\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>g)</h5>\n",
    "Se utiliza dropout a continuación del embeding y de la capa LSTM. Los resultados obtenidos tienen el mayor accuracy de validación de todos los casos estudiados. Esto en teoría, debería ser gracias a que el utilizar dropout disminuye la posibilidad de overfitting, ajustandose mejor a los datos de prueba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           96000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 149,301\n",
      "Trainable params: 149,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 294s - loss: 0.4938 - acc: 0.7517   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 289s - loss: 0.3433 - acc: 0.8574   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 289s - loss: 0.2748 - acc: 0.8902   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=3000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "top_words = 3000\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>h)</h5>\n",
    "Se utilizan distintas técnicas para mejorar los resultados, entre ellas está la cross validation, el cambio de ciertos parámetros, etc. El mejor resultado obtenido para las pruebas realizadas, se logra al aumentar el largo del verctor del embedding a 128. En el caso contrario, el peor resultado fue al usar relu como función de activación en la última capa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           96000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 149,301\n",
      "Trainable params: 149,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 301s - loss: 1.1096 - acc: 0.5984   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 307s - loss: 0.8880 - acc: 0.6617   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 310s - loss: 0.5093 - acc: 0.7789   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=3000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "top_words = 3000\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 50)           150000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 210,501\n",
      "Trainable params: 210,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 309s - loss: 0.4467 - acc: 0.7880   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 304s - loss: 0.3151 - acc: 0.8716   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 299s - loss: 0.2929 - acc: 0.8814   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=3000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "top_words = 3000\n",
    "embedding_vector_length = 50\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 128)          384000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 475,701\n",
      "Trainable params: 475,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 310s - loss: 0.4640 - acc: 0.7770   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 295s - loss: 0.3260 - acc: 0.8663   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 301s - loss: 0.3047 - acc: 0.8757   \n",
      "25000/25000 [==============================] - 142s   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=3000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "top_words = 3000\n",
    "embedding_vector_length = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 315s - loss: 0.4908 - acc: 0.7632   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 308s - loss: 0.3087 - acc: 0.8733   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 312s - loss: 0.2778 - acc: 0.8895   \n",
      "25000/25000 [==============================] - 135s   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=4000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "top_words = 4500\n",
    "embedding_vector_length = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 312s - loss: 0.4203 - acc: 0.8037   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 312s - loss: 0.2951 - acc: 0.8821   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 324s - loss: 0.3476 - acc: 0.8512   \n",
      "25000/25000 [==============================] - 138s   \n",
      "Acc: 87.508%\n",
      "acc: 87.51%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 306s - loss: 0.4641 - acc: 0.7748   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 312s - loss: 0.2896 - acc: 0.8827   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 312s - loss: 0.2716 - acc: 0.8916   \n",
      "25000/25000 [==============================] - 137s   \n",
      "Acc: 87.46%\n",
      "acc: 87.46%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 311s - loss: 0.4263 - acc: 0.8021   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 313s - loss: 0.3064 - acc: 0.8764   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 309s - loss: 0.2436 - acc: 0.9044   \n",
      "25000/25000 [==============================] - 137s   \n",
      "Acc: 87.12%\n",
      "acc: 87.12%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 319s - loss: 0.4169 - acc: 0.8079   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 322s - loss: 0.3033 - acc: 0.8766   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 317s - loss: 0.2501 - acc: 0.9027   \n",
      "25000/25000 [==============================] - 143s   \n",
      "Acc: 85.16%\n",
      "acc: 85.16%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 315s - loss: 0.4402 - acc: 0.7942   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 313s - loss: 0.3041 - acc: 0.8753   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 305s - loss: 0.2405 - acc: 0.9058   \n",
      "25000/25000 [==============================] - 136s   \n",
      "Acc: 87.716%\n",
      "acc: 87.72%\n",
      "86.99% (+/- 0.94%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=4000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "top_words = 4500\n",
    "embedding_vector_length = 128\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "kfold = KFold(7250, n_folds=5 , shuffle=True)\n",
    "cvscores = []\n",
    "\n",
    "for i, (train, test) in enumerate(kfold):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Acc: \" + str(scores[1] * 100) + \"%\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Felipe\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5800/5800 [==============================] - 75s - loss: 0.6170 - acc: 0.6434    \n",
      "Epoch 2/3\n",
      "5800/5800 [==============================] - 70s - loss: 0.3382 - acc: 0.8607    \n",
      "Epoch 3/3\n",
      "5800/5800 [==============================] - 70s - loss: 0.2566 - acc: 0.8984    \n",
      "1450/1450 [==============================] - 8s     \n",
      "Acc: 82.8275861905%\n",
      "acc: 82.83%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "5800/5800 [==============================] - 73s - loss: 0.5988 - acc: 0.6595    \n",
      "Epoch 2/3\n",
      "5800/5800 [==============================] - 71s - loss: 0.3442 - acc: 0.8605    \n",
      "Epoch 3/3\n",
      "5800/5800 [==============================] - 71s - loss: 0.2561 - acc: 0.8995    \n",
      "1450/1450 [==============================] - 8s     \n",
      "Acc: 81.4482758703%\n",
      "acc: 81.45%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "5800/5800 [==============================] - 72s - loss: 0.6776 - acc: 0.6078    \n",
      "Epoch 2/3\n",
      "5800/5800 [==============================] - 71s - loss: 0.4598 - acc: 0.8021    \n",
      "Epoch 3/3\n",
      "5800/5800 [==============================] - 71s - loss: 0.3132 - acc: 0.8726    \n",
      "1450/1450 [==============================] - 8s     \n",
      "Acc: 79.379310353%\n",
      "acc: 79.38%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "5800/5800 [==============================] - 71s - loss: 0.6075 - acc: 0.6631    \n",
      "Epoch 2/3\n",
      "5800/5800 [==============================] - 70s - loss: 0.3342 - acc: 0.8607    \n",
      "Epoch 3/3\n",
      "5800/5800 [==============================] - 70s - loss: 0.2330 - acc: 0.9083    \n",
      "1450/1450 [==============================] - 8s     \n",
      "Acc: 81.379310353%\n",
      "acc: 81.38%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 128)          576000    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 667,701\n",
      "Trainable params: 667,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "5800/5800 [==============================] - 72s - loss: 0.5688 - acc: 0.6997    \n",
      "Epoch 2/3\n",
      "5800/5800 [==============================] - 71s - loss: 0.3366 - acc: 0.8619    \n",
      "Epoch 3/3\n",
      "5800/5800 [==============================] - 71s - loss: 0.2289 - acc: 0.9134    \n",
      "1450/1450 [==============================] - 8s     \n",
      "Acc: 84.2068965353%\n",
      "acc: 84.21%\n",
      "81.85% (+/- 1.61%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "np.random.seed(3)\n",
    "from keras.preprocessing import sequence\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=4000, seed=15)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "top_words = 4500\n",
    "embedding_vector_length = 128\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "kfold = KFold(7250, n_folds=5 , shuffle=True)\n",
    "cvscores = []\n",
    "\n",
    "for i, (train, test) in enumerate(kfold):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(X_train[train], y_train[train], nb_epoch=3, batch_size=64)\n",
    "    scores = model.evaluate(X_train[test], y_train[test], verbose=1)\n",
    "    print(\"Acc: \" + str(scores[1] * 100) + \"%\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
